{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, ConcatDataset, Dataset\nfrom tqdm import tqdm\n\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom transformers import BertTokenizer, BertModel, AutoTokenizer\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:23:56.418757Z","iopub.execute_input":"2024-08-04T18:23:56.419635Z","iopub.status.idle":"2024-08-04T18:23:56.425288Z","shell.execute_reply.started":"2024-08-04T18:23:56.419597Z","shell.execute_reply":"2024-08-04T18:23:56.424418Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset('dair-ai/emotion',trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:23:56.692929Z","iopub.execute_input":"2024-08-04T18:23:56.693244Z","iopub.status.idle":"2024-08-04T18:23:58.599261Z","shell.execute_reply.started":"2024-08-04T18:23:56.693219Z","shell.execute_reply":"2024-08-04T18:23:58.598522Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=300)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:23:58.601006Z","iopub.execute_input":"2024-08-04T18:23:58.601431Z","iopub.status.idle":"2024-08-04T18:24:00.543500Z","shell.execute_reply.started":"2024-08-04T18:23:58.601396Z","shell.execute_reply":"2024-08-04T18:24:00.542720Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\ntrain_dataset = TensorDataset(tokenized_datasets['train']['input_ids'], tokenized_datasets['train']['attention_mask'], tokenized_datasets['train']['label'])\ntest_dataset = TensorDataset(tokenized_datasets['test']['input_ids'], tokenized_datasets['test']['attention_mask'], tokenized_datasets['test']['label'])\nval_dataset = TensorDataset(tokenized_datasets['validation']['input_ids'], tokenized_datasets['validation']['attention_mask'], tokenized_datasets['validation']['label'])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=8)\nval_loader = DataLoader(val_dataset, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.544865Z","iopub.execute_input":"2024-08-04T18:24:00.545216Z","iopub.status.idle":"2024-08-04T18:24:00.653451Z","shell.execute_reply.started":"2024-08-04T18:24:00.545183Z","shell.execute_reply":"2024-08-04T18:24:00.652681Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# from GPT\n'''class LoRA(nn.Module):\n    def __init__(self, model, r=4):\n        super(LoRA, self).__init__()\n        self.model = model\n        self.r = r\n        self.lora_layers = nn.ModuleDict()\n\n        # Initialize LoRA layers for BERT's attention weights\n        for name, param in self.model.named_parameters():\n            if 'attention' in name and 'weight' in name:\n                lora_layer = nn.Parameter(torch.zeros_like(param))\n                nn.init.kaiming_uniform_(lora_layer, a=0.5)\n                self.lora_layers[name] = lora_layer\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        \n        for name, param in self.model.named_parameters():\n            if name in self.lora_layers:\n                param.data += self.lora_layers[name] / self.r\n        \n        return outputs'''","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.655880Z","iopub.execute_input":"2024-08-04T18:24:00.656294Z","iopub.status.idle":"2024-08-04T18:24:00.662554Z","shell.execute_reply.started":"2024-08-04T18:24:00.656261Z","shell.execute_reply":"2024-08-04T18:24:00.661688Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"\"class LoRA(nn.Module):\\n    def __init__(self, model, r=4):\\n        super(LoRA, self).__init__()\\n        self.model = model\\n        self.r = r\\n        self.lora_layers = nn.ModuleDict()\\n\\n        # Initialize LoRA layers for BERT's attention weights\\n        for name, param in self.model.named_parameters():\\n            if 'attention' in name and 'weight' in name:\\n                lora_layer = nn.Parameter(torch.zeros_like(param))\\n                nn.init.kaiming_uniform_(lora_layer, a=0.5)\\n                self.lora_layers[name] = lora_layer\\n\\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\\n        outputs = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\\n        \\n        for name, param in self.model.named_parameters():\\n            if name in self.lora_layers:\\n                param.data += self.lora_layers[name] / self.r\\n        \\n        return outputs\""},"metadata":{}}]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self, num_labels):\n        super().__init__()\n        self.bert=BertModel.from_pretrained('bert-base-uncased')\n        self.fc1=nn.Linear(768, num_labels)\n\n    def forward(self, **inputs):\n        output=self.bert(**inputs)\n        cls_token_hidden_state = output.last_hidden_state[:, 0, :]\n        logits = self.fc1(cls_token_hidden_state)\n        return logits\n    \nmodel= BertClassifier(num_labels=6)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.663719Z","iopub.execute_input":"2024-08-04T18:24:00.664064Z","iopub.status.idle":"2024-08-04T18:24:00.920594Z","shell.execute_reply.started":"2024-08-04T18:24:00.664033Z","shell.execute_reply":"2024-08-04T18:24:00.919827Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"for param in model.bert.embeddings.parameters():\n    param.requires_grad = False\n\n# 12 out of 12 layers are freezed \nfor layer in model.bert.encoder.layer[:11]:\n    for param in layer.parameters():\n        param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.921886Z","iopub.execute_input":"2024-08-04T18:24:00.922312Z","iopub.status.idle":"2024-08-04T18:24:00.929718Z","shell.execute_reply.started":"2024-08-04T18:24:00.922278Z","shell.execute_reply":"2024-08-04T18:24:00.928763Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from prettytable import PrettyTable\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params+=params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\ncount_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.931028Z","iopub.execute_input":"2024-08-04T18:24:00.931383Z","iopub.status.idle":"2024-08-04T18:24:00.948623Z","shell.execute_reply.started":"2024-08-04T18:24:00.931350Z","shell.execute_reply":"2024-08-04T18:24:00.947824Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"+---------------------------------------------------------+------------+\n|                         Modules                         | Parameters |\n+---------------------------------------------------------+------------+\n|    bert.encoder.layer.11.attention.self.query.weight    |   589824   |\n|     bert.encoder.layer.11.attention.self.query.bias     |    768     |\n|     bert.encoder.layer.11.attention.self.key.weight     |   589824   |\n|      bert.encoder.layer.11.attention.self.key.bias      |    768     |\n|    bert.encoder.layer.11.attention.self.value.weight    |   589824   |\n|     bert.encoder.layer.11.attention.self.value.bias     |    768     |\n|   bert.encoder.layer.11.attention.output.dense.weight   |   589824   |\n|    bert.encoder.layer.11.attention.output.dense.bias    |    768     |\n| bert.encoder.layer.11.attention.output.LayerNorm.weight |    768     |\n|  bert.encoder.layer.11.attention.output.LayerNorm.bias  |    768     |\n|     bert.encoder.layer.11.intermediate.dense.weight     |  2359296   |\n|      bert.encoder.layer.11.intermediate.dense.bias      |    3072    |\n|        bert.encoder.layer.11.output.dense.weight        |  2359296   |\n|         bert.encoder.layer.11.output.dense.bias         |    768     |\n|      bert.encoder.layer.11.output.LayerNorm.weight      |    768     |\n|       bert.encoder.layer.11.output.LayerNorm.bias       |    768     |\n|                 bert.pooler.dense.weight                |   589824   |\n|                  bert.pooler.dense.bias                 |    768     |\n|                        fc1.weight                       |    4608    |\n|                         fc1.bias                        |     6      |\n+---------------------------------------------------------+------------+\nTotal Trainable Params: 7683078\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"7683078"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=2e-5)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.949739Z","iopub.execute_input":"2024-08-04T18:24:00.950027Z","iopub.status.idle":"2024-08-04T18:24:00.962099Z","shell.execute_reply.started":"2024-08-04T18:24:00.950003Z","shell.execute_reply":"2024-08-04T18:24:00.961453Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\nmodel = model.to('cuda')\nmodel.to(device)\n\nnum_epochs = 3\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_loader):\n        optimizer.zero_grad()\n        \n        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}')\n\n'''# validation\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            loss=criterion(outputs, labels)\n            predictions = torch.argmax(outputs, dim=1)\n            \n            total_correct += (predictions == labels).sum().item()\n            total_samples += labels.size(0)\n\n    accuracy = total_correct / total_samples\n    print(f'Test Accuracy: {accuracy:.4f}')\n    print(f'test loss: {loss:.4f}')'''","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:24:00.963289Z","iopub.execute_input":"2024-08-04T18:24:00.963561Z","iopub.status.idle":"2024-08-04T18:34:38.178762Z","shell.execute_reply.started":"2024-08-04T18:24:00.963538Z","shell.execute_reply":"2024-08-04T18:34:38.177829Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [03:32<00:00,  2.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Training Loss: 1.1755\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [03:32<00:00,  2.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3, Training Loss: 0.7274\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 500/500 [03:32<00:00,  2.35it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3, Training Loss: 0.6091\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"\"# validation\\n    model.eval()\\n    total_correct = 0\\n    total_samples = 0\\n\\n    with torch.no_grad():\\n        for batch in tqdm(val_loader):\\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\\n            \\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n            loss=criterion(outputs, labels)\\n            predictions = torch.argmax(outputs, dim=1)\\n            \\n            total_correct += (predictions == labels).sum().item()\\n            total_samples += labels.size(0)\\n\\n    accuracy = total_correct / total_samples\\n    print(f'Test Accuracy: {accuracy:.4f}')\\n    print(f'test loss: {loss:.4f}')\""},"metadata":{}}]},{"cell_type":"code","source":"# Save the model state dictionary\ntorch.save(model, 'bert_classifier_model.pt')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:34:50.576345Z","iopub.execute_input":"2024-08-04T18:34:50.576719Z","iopub.status.idle":"2024-08-04T18:34:51.167440Z","shell.execute_reply.started":"2024-08-04T18:34:50.576687Z","shell.execute_reply":"2024-08-04T18:34:51.166543Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\n# Example sentence\nsentence = \"As Sarah stood on the edge of the cliff, her heart pounded wildly in her chest. The view was breathtaking, with the sun setting over the horizon and casting a golden glow on the ocean waves below. She felt a rush of emotions, a mix of awe and anxiety. This was the place where she had first met him, where their love story had begun. But now, standing alone, the memories brought a tinge of fear. Would she ever feel that kind of love again? The wind whispered around her, as if urging her to hold on to hope, but the shadow of doubt loomed large, making her question if it was all just an illusion.\"\n\n# Tokenize the input sentence\ninputs = tokenizer(sentence, return_tensors='pt', padding='max_length', truncation=True, max_length=128)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:23:41.356435Z","iopub.status.idle":"2024-08-04T18:23:41.356794Z","shell.execute_reply.started":"2024-08-04T18:23:41.356610Z","shell.execute_reply":"2024-08-04T18:23:41.356625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs. to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:11:45.554465Z","iopub.execute_input":"2024-08-04T18:11:45.554821Z","iopub.status.idle":"2024-08-04T18:11:45.569917Z","shell.execute_reply.started":"2024-08-04T18:11:45.554794Z","shell.execute_reply":"2024-08-04T18:11:45.568925Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2004,  4532,  2768,  2006,  1996,  3341,  1997,  1996,  7656,\n          1010,  2014,  2540, 13750, 13544,  1999,  2014,  3108,  1012,  1996,\n          3193,  2001,  3052, 17904,  1010,  2007,  1996,  3103,  4292,  2058,\n          1996,  9154,  1998,  9179,  1037,  3585,  8652,  2006,  1996,  4153,\n          5975,  2917,  1012,  2016,  2371,  1037,  5481,  1997,  6699,  1010,\n          1037,  4666,  1997, 15180,  1998, 10089,  1012,  2023,  2001,  1996,\n          2173,  2073,  2016,  2018,  2034,  2777,  2032,  1010,  2073,  2037,\n          2293,  2466,  2018,  5625,  1012,  2021,  2085,  1010,  3061,  2894,\n          1010,  1996,  5758,  2716,  1037, 28642,  2063,  1997,  3571,  1012,\n          2052,  2016,  2412,  2514,  2008,  2785,  1997,  2293,  2153,  1029,\n          1996,  3612,  3990,  2105,  2014,  1010,  2004,  2065, 14328,  2014,\n          2000,  2907,  2006,  2000,  3246,  1010,  2021,  1996,  5192,  1997,\n          4797, 24358,  2312,  1010,  2437,  2014,  3160,   102]],\n       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"},"metadata":{}}]},{"cell_type":"code","source":"# Perform inference\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Get the predicted label\npredictions = torch.argmax(outputs, dim=1)\n\n# Map the predicted label to the emotion\nlabel_map = {\n    0: 'sadness',\n    1: 'joy',\n    2: 'love',\n    3: 'anger',\n    4: 'fear',\n    5: 'surprise'\n}\npredicted_emotion = label_map[predictions.item()]\n\nprint(f'The emotion predicted for the sentence \"{sentence}\" is: {predicted_emotion}')\nprint (predictions)\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:11:45.866028Z","iopub.execute_input":"2024-08-04T18:11:45.866363Z","iopub.status.idle":"2024-08-04T18:11:45.906024Z","shell.execute_reply.started":"2024-08-04T18:11:45.866333Z","shell.execute_reply":"2024-08-04T18:11:45.905108Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"The emotion predicted for the sentence \"As Sarah stood on the edge of the cliff, her heart pounded wildly in her chest. The view was breathtaking, with the sun setting over the horizon and casting a golden glow on the ocean waves below. She felt a rush of emotions, a mix of awe and anxiety. This was the place where she had first met him, where their love story had begun. But now, standing alone, the memories brought a tinge of fear. Would she ever feel that kind of love again? The wind whispered around her, as if urging her to hold on to hope, but the shadow of doubt loomed large, making her question if it was all just an illusion.\" is: fear\ntensor([4], device='cuda:0')\ntensor([[-0.2016,  0.6204, -1.6937, -1.5064,  3.2784, -0.7354]],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertClassifier(num_labels=6)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Load the fine-tuned model weights\nmodel.load_state_dict(torch.load(r'/kaggle/working/bert_classifier2.pt'), strict=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor)\n\nprint(\"\\nLoaded state_dict:\")\nloaded_state_dict = torch.load(r'/kaggle/working/bert_classifier.pt')\nfor param_tensor in loaded_state_dict:\n    print(param_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1=torch.load('/kaggle/working/bert_classifier_model.pt')\nmodel1","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:37:10.548195Z","iopub.execute_input":"2024-08-04T18:37:10.548613Z","iopub.status.idle":"2024-08-04T18:37:10.810718Z","shell.execute_reply.started":"2024-08-04T18:37:10.548582Z","shell.execute_reply":"2024-08-04T18:37:10.809802Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): BertClassifier(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSdpaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (fc1): Linear(in_features=768, out_features=6, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model1.eval()\n\n# Example sentence\nsentence = \"As Sarah stood on the edge of the cliff, her heart pounded wildly in her chest. The view was breathtaking, with the sun setting over the horizon and casting a golden glow on the ocean waves below. She felt a rush of emotions, a mix of awe and anxiety. This was the place where she had first met him, where their love story had begun. But now, standing alone, the memories brought a tinge of fear. Would she ever feel that kind of love again? The wind whispered around her, as if urging her to hold on to hope, but the shadow of doubt loomed large, making her question if it was all just an illusion.\"\n\n# Tokenize the input sentence\ninputs = tokenizer(sentence, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n\nwith torch.no_grad():\n    outputs = model1(**inputs)\n\n# Get the predicted label\npredictions = torch.argmax(outputs, dim=1)\n\n# Map the predicted label to the emotion\nlabel_map = {\n    0: 'sadness',\n    1: 'joy',\n    2: 'love',\n    3: 'anger',\n    4: 'fear',\n    5: 'surprise'\n}\npredicted_emotion = label_map[predictions.item()]\n\nprint(f'The emotion predicted for the sentence \"{sentence}\" is: {predicted_emotion}')\nprint (predictions)\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T18:38:08.979037Z","iopub.execute_input":"2024-08-04T18:38:08.979887Z","iopub.status.idle":"2024-08-04T18:38:09.029021Z","shell.execute_reply.started":"2024-08-04T18:38:08.979851Z","shell.execute_reply":"2024-08-04T18:38:09.028138Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"The emotion predicted for the sentence \"As Sarah stood on the edge of the cliff, her heart pounded wildly in her chest. The view was breathtaking, with the sun setting over the horizon and casting a golden glow on the ocean waves below. She felt a rush of emotions, a mix of awe and anxiety. This was the place where she had first met him, where their love story had begun. But now, standing alone, the memories brought a tinge of fear. Would she ever feel that kind of love again? The wind whispered around her, as if urging her to hold on to hope, but the shadow of doubt loomed large, making her question if it was all just an illusion.\" is: fear\ntensor([4], device='cuda:0')\ntensor([[-0.8899,  1.3056, -1.2069, -2.2569,  2.4537,  0.0702]],\n       device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
