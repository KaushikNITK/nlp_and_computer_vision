{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:36:47.805827Z","iopub.execute_input":"2024-10-06T10:36:47.806196Z","iopub.status.idle":"2024-10-06T10:36:52.868317Z","shell.execute_reply.started":"2024-10-06T10:36:47.806150Z","shell.execute_reply":"2024-10-06T10:36:52.867492Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"import images and make dataloader","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"timm/resisc45\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:36:52.870145Z","iopub.execute_input":"2024-10-06T10:36:52.870652Z","iopub.status.idle":"2024-10-06T10:37:01.684453Z","shell.execute_reply.started":"2024-10-06T10:36:52.870607Z","shell.execute_reply":"2024-10-06T10:37:01.683644Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5017e29dfee4790b11a2f9d9238d4e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7723e56dfc0a462aa6f10e1510b98e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/85.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c65c9b652f4055a10e1a623a7612d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/85.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f6eeb0bbff42e9a5d4e379a5981be4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/18900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96372bdc46ea41988e7717e09b2a2f53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/6300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50eaad4de8f2466682f2ae381b32eb49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e778aaac9084911836c006fc9c69ccc"}},"metadata":{}}]},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:37:01.685598Z","iopub.execute_input":"2024-10-06T10:37:01.686054Z","iopub.status.idle":"2024-10-06T10:37:01.693151Z","shell.execute_reply.started":"2024-10-06T10:37:01.686019Z","shell.execute_reply":"2024-10-06T10:37:01.692079Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label', 'image_id'],\n        num_rows: 18900\n    })\n    validation: Dataset({\n        features: ['image', 'label', 'image_id'],\n        num_rows: 6300\n    })\n    test: Dataset({\n        features: ['image', 'label', 'image_id'],\n        num_rows: 6300\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\n# Create a PyTorch Dataset\nclass CustomImageDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image = self.dataset[idx]['image']\n        label = self.dataset[idx]['label']\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# Create datasets for training and validation (assuming split exists)\ntrain_dataset = CustomImageDataset(ds['train'], transform=transform)\nval_dataset = CustomImageDataset(ds['validation'], transform=transform)\ntest_dataset = CustomImageDataset(ds['test'], transform=transform)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n\n# Example: Iterate through the training data\nfor images, labels in train_loader:\n    print(images.shape, labels.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:48:26.629124Z","iopub.execute_input":"2024-10-06T10:48:26.630168Z","iopub.status.idle":"2024-10-06T10:48:27.523373Z","shell.execute_reply.started":"2024-10-06T10:48:26.630121Z","shell.execute_reply":"2024-10-06T10:48:27.522283Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 256, 256]) torch.Size([64])\n","output_type":"stream"}]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, image_size=256, patch_size=16, in_channels=3, embed_dim=768):\n        super(PatchEmbedding, self).__init__()\n        self.patch_size = patch_size\n        self.grid_size = image_size // patch_size\n        self.embedding = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.position_embedding = nn.Parameter(torch.randn(1, self.grid_size**2 + 1, embed_dim))\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.embedding(x)  # (B, E, H', W')\n        x = x.flatten(2)  # (B, E, N)\n        x = x.transpose(1, 2)  # (B, N, E)\n        cls_token = self.position_embedding[:, 0, :].unsqueeze(1).expand(batch_size, 1, -1)\n        x = torch.cat((cls_token, x), dim=1)  # (B, N+1, E)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.out = nn.Linear(embed_dim, embed_dim)\n    \n    def forward(self, x):\n        B, N, E = x.size()\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn_scores = (q @ k.transpose(-2, -1)) * (1.0 / self.head_dim**0.5)\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        \n        out = attn_weights @ v\n        out = out.transpose(1, 2).reshape(B, N, E)\n        out = self.out(out)\n        return out\n\nclass FeedForward(nn.Module):\n    def __init__(self, embed_dim=768, ff_dim=3*768):\n        super(FeedForward, self).__init__()\n        self.fc1 = nn.Linear(embed_dim, ff_dim)\n        self.fc2 = nn.Linear(ff_dim, embed_dim)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12, ff_dim=768, dropout=0.1):\n        super(TransformerBlock, self).__init__()\n        self.attention = MultiHeadAttention(embed_dim, num_heads)\n        self.ffn = FeedForward(embed_dim, ff_dim)\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x2 = self.attention(x)\n        x = x + self.dropout(x2)\n        x = self.ln1(x)\n        x2 = self.ffn(x)\n        x = x + self.dropout(x2)\n        x = self.ln2(x)\n        return x\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, num_classes, image_size=256, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, ff_dim=3*768, num_layers=6):\n        super(VisionTransformer, self).__init__()\n        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n        ])\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.patch_embed(x)\n        for block in self.transformer_blocks:\n            x = block(x)\n        x = x[:, 0]  # Take the class token\n        x = self.mlp_head(x)\n        return x\n    \nmodel = VisionTransformer(num_classes=1000)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:58:13.451261Z","iopub.execute_input":"2024-10-06T10:58:13.452101Z","iopub.status.idle":"2024-10-06T10:58:13.778960Z","shell.execute_reply.started":"2024-10-06T10:58:13.452059Z","shell.execute_reply":"2024-10-06T10:58:13.778051Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"criterion=nn.CrossEntropyLoss()\noptimizer=optim.Adam(model.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:58:13.780728Z","iopub.execute_input":"2024-10-06T10:58:13.781447Z","iopub.status.idle":"2024-10-06T10:58:13.787628Z","shell.execute_reply.started":"2024-10-06T10:58:13.781401Z","shell.execute_reply":"2024-10-06T10:58:13.786656Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from prettytable import PrettyTable\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params+=params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params\ncount_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:58:13.869387Z","iopub.execute_input":"2024-10-06T10:58:13.869696Z","iopub.status.idle":"2024-10-06T10:58:13.890027Z","shell.execute_reply.started":"2024-10-06T10:58:13.869663Z","shell.execute_reply":"2024-10-06T10:58:13.889099Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"+-------------------------------------------+------------+\n|                  Modules                  | Parameters |\n+-------------------------------------------+------------+\n|       patch_embed.position_embedding      |   197376   |\n|        patch_embed.embedding.weight       |   589824   |\n|         patch_embed.embedding.bias        |    768     |\n| transformer_blocks.0.attention.qkv.weight |  1769472   |\n|  transformer_blocks.0.attention.qkv.bias  |    2304    |\n| transformer_blocks.0.attention.out.weight |   589824   |\n|  transformer_blocks.0.attention.out.bias  |    768     |\n|    transformer_blocks.0.ffn.fc1.weight    |  1769472   |\n|     transformer_blocks.0.ffn.fc1.bias     |    2304    |\n|    transformer_blocks.0.ffn.fc2.weight    |  1769472   |\n|     transformer_blocks.0.ffn.fc2.bias     |    768     |\n|      transformer_blocks.0.ln1.weight      |    768     |\n|       transformer_blocks.0.ln1.bias       |    768     |\n|      transformer_blocks.0.ln2.weight      |    768     |\n|       transformer_blocks.0.ln2.bias       |    768     |\n| transformer_blocks.1.attention.qkv.weight |  1769472   |\n|  transformer_blocks.1.attention.qkv.bias  |    2304    |\n| transformer_blocks.1.attention.out.weight |   589824   |\n|  transformer_blocks.1.attention.out.bias  |    768     |\n|    transformer_blocks.1.ffn.fc1.weight    |  1769472   |\n|     transformer_blocks.1.ffn.fc1.bias     |    2304    |\n|    transformer_blocks.1.ffn.fc2.weight    |  1769472   |\n|     transformer_blocks.1.ffn.fc2.bias     |    768     |\n|      transformer_blocks.1.ln1.weight      |    768     |\n|       transformer_blocks.1.ln1.bias       |    768     |\n|      transformer_blocks.1.ln2.weight      |    768     |\n|       transformer_blocks.1.ln2.bias       |    768     |\n| transformer_blocks.2.attention.qkv.weight |  1769472   |\n|  transformer_blocks.2.attention.qkv.bias  |    2304    |\n| transformer_blocks.2.attention.out.weight |   589824   |\n|  transformer_blocks.2.attention.out.bias  |    768     |\n|    transformer_blocks.2.ffn.fc1.weight    |  1769472   |\n|     transformer_blocks.2.ffn.fc1.bias     |    2304    |\n|    transformer_blocks.2.ffn.fc2.weight    |  1769472   |\n|     transformer_blocks.2.ffn.fc2.bias     |    768     |\n|      transformer_blocks.2.ln1.weight      |    768     |\n|       transformer_blocks.2.ln1.bias       |    768     |\n|      transformer_blocks.2.ln2.weight      |    768     |\n|       transformer_blocks.2.ln2.bias       |    768     |\n| transformer_blocks.3.attention.qkv.weight |  1769472   |\n|  transformer_blocks.3.attention.qkv.bias  |    2304    |\n| transformer_blocks.3.attention.out.weight |   589824   |\n|  transformer_blocks.3.attention.out.bias  |    768     |\n|    transformer_blocks.3.ffn.fc1.weight    |  1769472   |\n|     transformer_blocks.3.ffn.fc1.bias     |    2304    |\n|    transformer_blocks.3.ffn.fc2.weight    |  1769472   |\n|     transformer_blocks.3.ffn.fc2.bias     |    768     |\n|      transformer_blocks.3.ln1.weight      |    768     |\n|       transformer_blocks.3.ln1.bias       |    768     |\n|      transformer_blocks.3.ln2.weight      |    768     |\n|       transformer_blocks.3.ln2.bias       |    768     |\n| transformer_blocks.4.attention.qkv.weight |  1769472   |\n|  transformer_blocks.4.attention.qkv.bias  |    2304    |\n| transformer_blocks.4.attention.out.weight |   589824   |\n|  transformer_blocks.4.attention.out.bias  |    768     |\n|    transformer_blocks.4.ffn.fc1.weight    |  1769472   |\n|     transformer_blocks.4.ffn.fc1.bias     |    2304    |\n|    transformer_blocks.4.ffn.fc2.weight    |  1769472   |\n|     transformer_blocks.4.ffn.fc2.bias     |    768     |\n|      transformer_blocks.4.ln1.weight      |    768     |\n|       transformer_blocks.4.ln1.bias       |    768     |\n|      transformer_blocks.4.ln2.weight      |    768     |\n|       transformer_blocks.4.ln2.bias       |    768     |\n| transformer_blocks.5.attention.qkv.weight |  1769472   |\n|  transformer_blocks.5.attention.qkv.bias  |    2304    |\n| transformer_blocks.5.attention.out.weight |   589824   |\n|  transformer_blocks.5.attention.out.bias  |    768     |\n|    transformer_blocks.5.ffn.fc1.weight    |  1769472   |\n|     transformer_blocks.5.ffn.fc1.bias     |    2304    |\n|    transformer_blocks.5.ffn.fc2.weight    |  1769472   |\n|     transformer_blocks.5.ffn.fc2.bias     |    768     |\n|      transformer_blocks.5.ln1.weight      |    768     |\n|       transformer_blocks.5.ln1.bias       |    768     |\n|      transformer_blocks.5.ln2.weight      |    768     |\n|       transformer_blocks.5.ln2.bias       |    768     |\n|             mlp_head.0.weight             |    768     |\n|              mlp_head.0.bias              |    768     |\n|             mlp_head.1.weight             |   768000   |\n|              mlp_head.1.bias              |    1000    |\n+-------------------------------------------+------------+\nTotal Trainable Params: 37003240\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"37003240"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else torch.device('cpu')\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\nmodel = model.to(device)\nmodel.to(device)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    # Training loop\n    for images, labels in tqdm(train_loader):\n\n        images, labels = images.to(device,non_blocking=True), labels.to(device,non_blocking=True)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Calculate accuracy\n#     model.eval()\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for images, labels in train_loader:\n#             images, labels = images.to(device,non_blocking=True), labels.to(device,non_blocking=True)\n#             outputs = model(images)\n#             _, predicted = torch.max(outputs.data, 1)\n#             total += labels.size(0)\n#             correct += (predicted == labels).sum().item()\n\n#         accuracy = 100 * correct / total\n#         print(f'Epoch: {epoch+1}/{num_epochs}, Accuracy: {accuracy:.2f}%')\n\n\n    model.eval()\n    correct = 0\n    total = 0\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device,non_blocking=True), labels.to(device,non_blocking=True)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Accumulate the validation loss\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    # Compute average validation loss\n    average_val_loss = val_loss / len(val_loader)\n    accuracy = 100 * correct / total\n\n    print(f'Epoch: {epoch+1}/{num_epochs}, Accuracy: {accuracy:.2f}%, Loss: {average_val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T10:58:14.225541Z","iopub.execute_input":"2024-10-06T10:58:14.226617Z","iopub.status.idle":"2024-10-06T11:34:29.070412Z","shell.execute_reply.started":"2024-10-06T10:58:14.226566Z","shell.execute_reply":"2024-10-06T11:34:29.069139Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:14<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/10, Accuracy: 38.48%, Loss: 2.1417\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:14<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2/10, Accuracy: 57.35%, Loss: 1.4431\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:14<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3/10, Accuracy: 62.41%, Loss: 1.2521\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:14<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4/10, Accuracy: 64.83%, Loss: 1.1681\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:14<00:00,  1.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5/10, Accuracy: 65.43%, Loss: 1.1507\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:13<00:00,  1.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6/10, Accuracy: 66.87%, Loss: 1.0781\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:13<00:00,  1.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7/10, Accuracy: 67.71%, Loss: 1.0528\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:13<00:00,  1.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8/10, Accuracy: 69.56%, Loss: 1.0106\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:13<00:00,  1.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9/10, Accuracy: 67.32%, Loss: 1.1239\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 296/296 [03:13<00:00,  1.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10/10, Accuracy: 68.37%, Loss: 1.1214\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For increasing accuracy \n\n1. number of epochs need to increased with change in learning rate\n2. increases number of layers of transformer \n3. larger dataset would always be helpful that means augmentation is a lot helpful (as transformers are learning from 16x16 patches it learns diferently if you rotate some image by some degree)","metadata":{}},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'Test Accuracy: {accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:35:09.945512Z","iopub.execute_input":"2024-10-06T11:35:09.945934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}