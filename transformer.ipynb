{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:22.876635Z","iopub.status.busy":"2024-09-20T12:57:22.875854Z","iopub.status.idle":"2024-09-20T12:57:28.693719Z","shell.execute_reply":"2024-09-20T12:57:28.692968Z","shell.execute_reply.started":"2024-09-20T12:57:22.876582Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, TensorDataset, random_split, ConcatDataset, Dataset\n","from tqdm import tqdm\n","\n","from torch.nn.utils.rnn import pad_sequence\n","\n","from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:28.695931Z","iopub.status.busy":"2024-09-20T12:57:28.695502Z","iopub.status.idle":"2024-09-20T12:57:29.631774Z","shell.execute_reply":"2024-09-20T12:57:29.630733Z","shell.execute_reply.started":"2024-09-20T12:57:28.695897Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:29.633667Z","iopub.status.busy":"2024-09-20T12:57:29.633092Z","iopub.status.idle":"2024-09-20T12:57:31.950670Z","shell.execute_reply":"2024-09-20T12:57:31.949645Z","shell.execute_reply.started":"2024-09-20T12:57:29.633631Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36a4f956b3c14dc7b3bf6dae22678c18","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e08ffeb12f4b4cf8b08c8c3a23b0057e","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f0602210f7a4ebb88ddbe3054a4d0eb","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/419k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b45494c879f04de2a81722051b304258","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1a6e1f8629147318f3d86f000e48f98","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ds = load_dataset(\"openai/gsm8k\", \"main\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:31.953417Z","iopub.status.busy":"2024-09-20T12:57:31.952952Z","iopub.status.idle":"2024-09-20T12:57:31.960578Z","shell.execute_reply":"2024-09-20T12:57:31.959672Z","shell.execute_reply.started":"2024-09-20T12:57:31.953382Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'answer'],\n","        num_rows: 7473\n","    })\n","    test: Dataset({\n","        features: ['question', 'answer'],\n","        num_rows: 1319\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["ds"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:31.962077Z","iopub.status.busy":"2024-09-20T12:57:31.961748Z","iopub.status.idle":"2024-09-20T12:57:31.974896Z","shell.execute_reply":"2024-09-20T12:57:31.973994Z","shell.execute_reply.started":"2024-09-20T12:57:31.962042Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n"," 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["ds['train'][0]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:31.976418Z","iopub.status.busy":"2024-09-20T12:57:31.976056Z","iopub.status.idle":"2024-09-20T12:57:32.027418Z","shell.execute_reply":"2024-09-20T12:57:32.026610Z","shell.execute_reply.started":"2024-09-20T12:57:31.976377Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available(): \n","    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.028850Z","iopub.status.busy":"2024-09-20T12:57:32.028548Z","iopub.status.idle":"2024-09-20T12:57:32.042070Z","shell.execute_reply":"2024-09-20T12:57:32.041099Z","shell.execute_reply.started":"2024-09-20T12:57:32.028818Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, emb_size, heads):\n","        super(MultiHeadAttention,self).__init__()\n","        self.heads=heads\n","        self.emb_size=emb_size\n","        self.head_dim=self.emb_size//self.heads\n","        self.w_k=nn.Linear(emb_size,emb_size)\n","        self.w_q=nn.Linear(emb_size,emb_size)\n","        self.w_v=nn.Linear(emb_size,emb_size)\n","        self.out=nn.Linear(emb_size,emb_size)\n","\n","        assert(self.head_dim * heads == emb_size),\"embeding size is not divisible by number of heads\"\n","\n","\n","    def forward(self,k,q,v,mask=None):\n","        N=q.shape[0]  # batch size\n","        K=self.w_k(k)\n","        Q=self.w_q(q)\n","        V=self.w_v(v)\n","\n","        K=K.view(N,K.shape[1],self.heads,self.head_dim).transpose(1,2)    # (batch size, sequence len, heads, head dimention)\n","        Q=Q.view(N,Q.shape[1],self.heads,self.head_dim).transpose(1,2)    # transposed to give(batch size, heads, sequence len, head dimention)\n","        V=V.view(N,V.shape[1],self.heads,self.head_dim).transpose(1,2)\n","\n","        attention=(torch.matmul(Q,K.transpose(-2,-1)))/torch.tensor(self.head_dim**0.5)\n","        \n","        if mask is not None:\n","            mask=mask.reshape(-1,1,1,1024)\n","            attention.masked_fill_(mask==0, -1e9)\n","\n","        attention_scores=F.softmax(attention, dim=-1)\n","        output=torch.matmul(attention_scores,V)\n","        output = output.transpose(1, 2).reshape(N, -1, self.emb_size)\n","        output=self.out(output)\n","\n","        return output"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.044361Z","iopub.status.busy":"2024-09-20T12:57:32.043592Z","iopub.status.idle":"2024-09-20T12:57:32.053590Z","shell.execute_reply":"2024-09-20T12:57:32.052821Z","shell.execute_reply.started":"2024-09-20T12:57:32.044318Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, heads, emb_size):\n","        super(Encoder, self).__init__()\n","        self.mha=MultiHeadAttention(emb_size, heads)\n","        self.ff1=nn.Linear(emb_size,2*emb_size)\n","        self.ff2=nn.Linear(2*emb_size, emb_size)\n","        self.norm1=nn.LayerNorm(emb_size)\n","        self.norm2=nn.LayerNorm(emb_size)\n","        self.dropout=nn.Dropout(p=0.2)\n","\n","    def forward(self, x, mask=None):\n","        attention_out=self.mha(x,x,x,mask)\n","        attention_out = self.dropout(attention_out)\n","        out1=self.norm1(x+attention_out)\n","\n","        ff_out=F.relu(self.ff1(out1))\n","        ff_out=self.ff2(ff_out)\n","        out2=self.dropout(ff_out)\n","        encoder_out=self.norm2(out1+out2)\n","        return encoder_out"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.055525Z","iopub.status.busy":"2024-09-20T12:57:32.055018Z","iopub.status.idle":"2024-09-20T12:57:32.065808Z","shell.execute_reply":"2024-09-20T12:57:32.064906Z","shell.execute_reply.started":"2024-09-20T12:57:32.055483Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, heads, emb_size):\n","        super(Decoder, self).__init__()\n","        self.mmha=MultiHeadAttention(emb_size, heads)\n","        self.mha=MultiHeadAttention(emb_size, heads)\n","        self.ff1=nn.Linear(emb_size,2*emb_size)\n","        self.ff2=nn.Linear(2*emb_size, emb_size)\n","        self.norm1=nn.LayerNorm(emb_size)\n","        self.norm2=nn.LayerNorm(emb_size)\n","        self.norm3=nn.LayerNorm(emb_size)\n","        self.dropout=nn.Dropout(p=0.2)\n","\n","    def forward(self, x, encoder_out, source_mask, target_mask):\n","        mask_attention_out=self.mmha(x,x,x,target_mask)\n","        mask_attention_out=self.dropout(mask_attention_out)\n","        out1=self.norm1(x+mask_attention_out)\n","\n","        enc_dec_attention_out=self.mha(encoder_out,out1,encoder_out)\n","        enc_dec_attention_out=self.dropout(enc_dec_attention_out)\n","        out2=self.norm2(out1+enc_dec_attention_out)\n","\n","        ff_output=F.relu(self.ff1(out2))\n","        ff_output=self.ff2(ff_output)\n","        ff_output=self.dropout(ff_output)\n","        out3=self.norm3(out2+ff_output)\n","\n","        return out3"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.068633Z","iopub.status.busy":"2024-09-20T12:57:32.068329Z","iopub.status.idle":"2024-09-20T12:57:32.076982Z","shell.execute_reply":"2024-09-20T12:57:32.076148Z","shell.execute_reply.started":"2024-09-20T12:57:32.068603Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(nn.Module):\n","    def __init__(self, seq_len, emb_size, n=10000):\n","        super(PositionalEmbedding, self).__init__()\n","        self.embedding = self.create_positional_embedding(seq_len, emb_size, n)\n","\n","    def create_positional_embedding(self, seq_len, emb_size, n):\n","        P = np.zeros((seq_len, emb_size))\n","        for pos in range(seq_len):\n","            for i in range(emb_size // 2):\n","                denominator = np.power(n, 2 * i / emb_size)\n","                P[pos, 2 * i] = np.sin(pos / denominator)\n","                P[pos, 2 * i + 1] = np.cos(pos / denominator)\n","        return torch.tensor(P, dtype=torch.float32)\n","\n","    def forward(self, idx):\n","        pos_idx=self.embedding\n","        pos_idx=pos_idx.to(device)\n","        idx=idx+pos_idx\n","        return idx"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.078232Z","iopub.status.busy":"2024-09-20T12:57:32.077953Z","iopub.status.idle":"2024-09-20T12:57:32.088249Z","shell.execute_reply":"2024-09-20T12:57:32.087364Z","shell.execute_reply.started":"2024-09-20T12:57:32.078202Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, vocab_size, input_dim, emb_size, num_encoder_layers, num_decoder_layers, heads, seq_len):\n","        super(Transformer, self).__init__()\n","        \n","        self.embedding=nn.Embedding(vocab_size, emb_size)\n","        self.encoder_layers = nn.ModuleList([Encoder(heads, emb_size) for _ in range(num_encoder_layers)])\n","        self.decoder_layers = nn.ModuleList([Decoder(heads, emb_size) for _ in range(num_decoder_layers)])\n","        self.position_encodings = PositionalEmbedding(seq_len, emb_size)\n","        self.linear = nn.Linear(emb_size, vocab_size)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","\n","        src=self.embedding(src)   # need to do *sqrt(d_model)\n","        tgt=self.embedding(tgt)   # for transulation task give different embedding\n","\n","        src=self.position_encodings(src)\n","        tgt=self.position_encodings(tgt)\n","\n","        for encoder in self.encoder_layers:\n","            src = encoder(src, src_mask)\n","        \n","        for decoder in self.decoder_layers:\n","            tgt = decoder(tgt, src, src_mask, tgt_mask)\n","\n","        output = self.linear(tgt)\n","        output = F.softmax(output, dim=-1)\n","        \n","        return output\n","    \n","\n","\n","input_dim = 1000\n","emb_size = 512\n","heads = 8\n","num_encoder_layers = 6\n","num_decoder_layers = 6\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.089577Z","iopub.status.busy":"2024-09-20T12:57:32.089298Z","iopub.status.idle":"2024-09-20T12:57:32.797942Z","shell.execute_reply":"2024-09-20T12:57:32.797006Z","shell.execute_reply.started":"2024-09-20T12:57:32.089547Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42e39b061615407d9f7dd5275b5c05bb","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5feaa2d778e456ebdf3a045e0fce53d","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96e69402f6fa4992bb925a18c2dd4262","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cda66dfeb5743938e52973d72839142","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-20T12:57:32.799534Z","iopub.status.busy":"2024-09-20T12:57:32.799156Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58aafa3305ae4ead9c8f128652a97ca2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Preprocess data\n","def preprocess_function(examples):\n","    # Combine question and answer for input\n","    inputs = [f\"question: {q} answer:\" for q in examples['question']]\n","    targets = examples['answer']\n","    \n","    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=1024)\n","    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=1024)\n","    \n","    model_inputs['labels'] = labels['input_ids']\n","    return model_inputs\n","\n","# Split dataset\n","train_data = ds['train'].map(preprocess_function, batched=True) \n","test_data = ds['test'].map(preprocess_function, batched=True)\n","\n","\n","class QNADataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': torch.tensor(self.data[idx]['input_ids']),\n","            'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n","            'labels': torch.tensor(self.data[idx]['labels'])\n","        }\n","\n","train_dataset = QNADataset(train_data)\n","test_dataset = QNADataset(test_data)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=4)\n","validation_dataloader = DataLoader(test_dataset, batch_size=8, pin_memory=True, num_workers=4)\n","\n","model = Transformer(tokenizer.vocab_size, input_dim, emb_size, num_encoder_layers, num_decoder_layers, heads, seq_len=1024)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","optimizer = optim.Adam(model.parameters(), lr=3e-5)\n","\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","if torch.cuda.device_count() > 1:\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","model = model.to('cuda')\n","model.to(device)\n","\n","num_epochs=3\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for batch in tqdm(train_dataloader):\n","        optimizer.zero_grad()\n","\n","        # Move tensors to the specified device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass\n","\n","        logits = model(src=input_ids, tgt=labels, src_mask=attention_mask)\n","        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    avg_loss = epoch_loss / len(train_dataloader)\n","    print(f\"training : Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","    \n","    model.eval()\n","    epoch_loss = 0\n","    for batch in tqdm(validation_dataloader):\n","\n","        # Move tensors to the specified device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass\n","        logits = model(src=input_ids, tgt=labels, src_mask=attention_mask)\n","        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","\n","        epoch_loss += loss.item()\n","\n","    avg_loss = epoch_loss / len(validation_dataloader)\n","    print(f\"validation : Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
