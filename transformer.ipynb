{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, ConcatDataset, Dataset\nfrom tqdm import tqdm\n\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom transformers import BertTokenizer, BertModel","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:01.286447Z","iopub.execute_input":"2024-10-05T08:50:01.287266Z","iopub.status.idle":"2024-10-05T08:50:07.861736Z","shell.execute_reply.started":"2024-10-05T08:50:01.287220Z","shell.execute_reply":"2024-10-05T08:50:07.860715Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:07.863561Z","iopub.execute_input":"2024-10-05T08:50:07.864024Z","iopub.status.idle":"2024-10-05T08:50:08.905851Z","shell.execute_reply.started":"2024-10-05T08:50:07.863988Z","shell.execute_reply":"2024-10-05T08:50:08.905023Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ds = load_dataset(\"openai/gsm8k\", \"main\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:08.907099Z","iopub.execute_input":"2024-10-05T08:50:08.907569Z","iopub.status.idle":"2024-10-05T08:50:12.023539Z","shell.execute_reply.started":"2024-10-05T08:50:08.907535Z","shell.execute_reply":"2024-10-05T08:50:12.022408Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4211f2dda47b423494527d78699de4f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c6e0b9c72214b4faaaa8e46d4c4ccec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec87c68063743b9b01998b92a00d4a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4583320f68fa456ea855492ff2ae32f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46c0153dd87e48fc849a025171128ee1"}},"metadata":{}}]},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.026682Z","iopub.execute_input":"2024-10-05T08:50:12.027662Z","iopub.status.idle":"2024-10-05T08:50:12.035796Z","shell.execute_reply.started":"2024-10-05T08:50:12.027606Z","shell.execute_reply":"2024-10-05T08:50:12.034643Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 7473\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 1319\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"ds['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.037508Z","iopub.execute_input":"2024-10-05T08:50:12.037961Z","iopub.status.idle":"2024-10-05T08:50:12.055623Z","shell.execute_reply.started":"2024-10-05T08:50:12.037892Z","shell.execute_reply":"2024-10-05T08:50:12.054513Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"},"metadata":{}}]},{"cell_type":"code","source":"if torch.cuda.is_available(): \n    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.057041Z","iopub.execute_input":"2024-10-05T08:50:12.057465Z","iopub.status.idle":"2024-10-05T08:50:12.121310Z","shell.execute_reply.started":"2024-10-05T08:50:12.057417Z","shell.execute_reply":"2024-10-05T08:50:12.120248Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Multi-Head Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, emb_size, heads):\n        super(MultiHeadAttention,self).__init__()\n        self.heads=heads\n        self.emb_size=emb_size\n        self.head_dim=self.emb_size//self.heads\n        self.w_k=nn.Linear(emb_size,emb_size)\n        self.w_q=nn.Linear(emb_size,emb_size)\n        self.w_v=nn.Linear(emb_size,emb_size)\n        self.out=nn.Linear(emb_size,emb_size)\n\n        assert(self.head_dim * heads == emb_size),\"embeding size is not divisible by number of heads\"\n\n\n    def forward(self,k,q,v,mask=None):\n        N=q.shape[0]  # batch size\n        K=self.w_k(k)\n        Q=self.w_q(q)\n        V=self.w_v(v)\n\n        K=K.view(N,K.shape[1],self.heads,self.head_dim).transpose(1,2)    # (batch size, sequence len, heads, head dimention)\n        Q=Q.view(N,Q.shape[1],self.heads,self.head_dim).transpose(1,2)    # transposed to give(batch size, heads, sequence len, head dimention)\n        V=V.view(N,V.shape[1],self.heads,self.head_dim).transpose(1,2)\n\n        attention=(torch.matmul(Q,K.transpose(-2,-1)))/torch.tensor(self.head_dim**0.5)\n        \n        if mask is not None:\n            mask=mask.reshape(-1,1,1,1024)\n            attention.masked_fill_(mask==0, -1e9)\n\n        attention_scores=F.softmax(attention, dim=-1)\n        output=torch.matmul(attention_scores,V)\n        output = output.transpose(1, 2).reshape(N, -1, self.emb_size)\n        output=self.out(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.123213Z","iopub.execute_input":"2024-10-05T08:50:12.123593Z","iopub.status.idle":"2024-10-05T08:50:12.137767Z","shell.execute_reply.started":"2024-10-05T08:50:12.123549Z","shell.execute_reply":"2024-10-05T08:50:12.136789Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Transformer architecture","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, heads, emb_size):\n        super(Encoder, self).__init__()\n        self.mha=MultiHeadAttention(emb_size, heads)\n        self.ff1=nn.Linear(emb_size,2*emb_size)\n        self.ff2=nn.Linear(2*emb_size, emb_size)\n        self.norm1=nn.LayerNorm(emb_size)\n        self.norm2=nn.LayerNorm(emb_size)\n        self.dropout=nn.Dropout(p=0.2)\n\n    def forward(self, x, mask=None):\n        attention_out=self.mha(x,x,x,mask)\n        attention_out = self.dropout(attention_out)\n        out1=self.norm1(x+attention_out)\n\n        ff_out=F.relu(self.ff1(out1))\n        ff_out=self.ff2(ff_out)\n        out2=self.dropout(ff_out)\n        encoder_out=self.norm2(out1+out2)\n        return encoder_out","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.140579Z","iopub.execute_input":"2024-10-05T08:50:12.140913Z","iopub.status.idle":"2024-10-05T08:50:12.155328Z","shell.execute_reply.started":"2024-10-05T08:50:12.140859Z","shell.execute_reply":"2024-10-05T08:50:12.154433Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, heads, emb_size):\n        super(Decoder, self).__init__()\n        self.mmha=MultiHeadAttention(emb_size, heads)\n        self.mha=MultiHeadAttention(emb_size, heads)\n        self.ff1=nn.Linear(emb_size,2*emb_size)\n        self.ff2=nn.Linear(2*emb_size, emb_size)\n        self.norm1=nn.LayerNorm(emb_size)\n        self.norm2=nn.LayerNorm(emb_size)\n        self.norm3=nn.LayerNorm(emb_size)\n        self.dropout=nn.Dropout(p=0.2)\n\n    def forward(self, x, encoder_out, source_mask, target_mask):\n        mask_attention_out=self.mmha(x,x,x,target_mask)\n        mask_attention_out=self.dropout(mask_attention_out)\n        out1=self.norm1(x+mask_attention_out)\n\n        enc_dec_attention_out=self.mha(encoder_out,out1,encoder_out)\n        enc_dec_attention_out=self.dropout(enc_dec_attention_out)\n        out2=self.norm2(out1+enc_dec_attention_out)\n\n        ff_output=F.relu(self.ff1(out2))\n        ff_output=self.ff2(ff_output)\n        ff_output=self.dropout(ff_output)\n        out3=self.norm3(out2+ff_output)\n\n        return out3","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.156682Z","iopub.execute_input":"2024-10-05T08:50:12.157748Z","iopub.status.idle":"2024-10-05T08:50:12.169424Z","shell.execute_reply.started":"2024-10-05T08:50:12.157687Z","shell.execute_reply":"2024-10-05T08:50:12.168546Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self, seq_len, emb_size, n=10000):\n        super(PositionalEmbedding, self).__init__()\n        self.embedding = self.create_positional_embedding(seq_len, emb_size, n)\n\n    def create_positional_embedding(self, seq_len, emb_size, n):\n        P = np.zeros((seq_len, emb_size))\n        for pos in range(seq_len):\n            for i in range(emb_size // 2):\n                denominator = np.power(n, 2 * i / emb_size)\n                P[pos, 2 * i] = np.sin(pos / denominator)\n                P[pos, 2 * i + 1] = np.cos(pos / denominator)\n        return torch.tensor(P, dtype=torch.float32)\n\n    def forward(self, idx):\n        pos_idx=self.embedding\n        pos_idx=pos_idx.to(device)\n        idx=idx+pos_idx\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.172541Z","iopub.execute_input":"2024-10-05T08:50:12.172905Z","iopub.status.idle":"2024-10-05T08:50:12.186686Z","shell.execute_reply.started":"2024-10-05T08:50:12.172842Z","shell.execute_reply":"2024-10-05T08:50:12.185595Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, vocab_size, input_dim, emb_size, num_encoder_layers, num_decoder_layers, heads, seq_len):\n        super(Transformer, self).__init__()\n        \n        self.embedding=nn.Embedding(vocab_size, emb_size)\n        self.encoder_layers = nn.ModuleList([Encoder(heads, emb_size) for _ in range(num_encoder_layers)])\n        self.decoder_layers = nn.ModuleList([Decoder(heads, emb_size) for _ in range(num_decoder_layers)])\n        self.position_encodings = PositionalEmbedding(seq_len, emb_size)\n        self.linear = nn.Linear(emb_size, vocab_size)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n\n        src=self.embedding(src)   # need to do *sqrt(d_model)\n        tgt=self.embedding(tgt)   # for transulation task give different embedding\n\n        src=self.position_encodings(src)\n        tgt=self.position_encodings(tgt)\n\n        for encoder in self.encoder_layers:\n            src = encoder(src, src_mask)\n        \n        for decoder in self.decoder_layers:\n            tgt = decoder(tgt, src, src_mask, tgt_mask)\n\n        output = self.linear(tgt)\n        output = F.softmax(output, dim=-1)\n        \n        return output\n    \n\n\ninput_dim = 1000\nemb_size = 512\nheads = 8\nnum_encoder_layers = 6\nnum_decoder_layers = 6\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.188235Z","iopub.execute_input":"2024-10-05T08:50:12.188643Z","iopub.status.idle":"2024-10-05T08:50:12.202425Z","shell.execute_reply.started":"2024-10-05T08:50:12.188599Z","shell.execute_reply":"2024-10-05T08:50:12.201528Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:12.203681Z","iopub.execute_input":"2024-10-05T08:50:12.204054Z","iopub.status.idle":"2024-10-05T08:50:13.411390Z","shell.execute_reply.started":"2024-10-05T08:50:12.204008Z","shell.execute_reply":"2024-10-05T08:50:13.410287Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37aeb329da34117973a7398f2a26440"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac454e1df96f458c85ce3d845e9c657a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"027413f42dc54772830f9b8f0e22caa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32461acb600447ed9cf9be460253414e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocess data\ndef preprocess_function(examples):\n    # Combine question and answer for input\n    inputs = [f\"question: {q} answer:\" for q in examples['question']]\n    targets = examples['answer']\n    \n    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=1024)\n    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=1024)\n    \n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\n# Split dataset\ntrain_data = ds['train'].map(preprocess_function, batched=True) \ntest_data = ds['test'].map(preprocess_function, batched=True)\n\n\nclass QNADataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.data[idx]['input_ids']),\n            'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n            'labels': torch.tensor(self.data[idx]['labels'])\n        }\n\ntrain_dataset = QNADataset(train_data)\ntest_dataset = QNADataset(test_data)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=4)\nvalidation_dataloader = DataLoader(test_dataset, batch_size=8, pin_memory=True, num_workers=4)\n\nmodel = Transformer(tokenizer.vocab_size, input_dim, emb_size, num_encoder_layers, num_decoder_layers, heads, seq_len=1024)\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = optim.Adam(model.parameters(), lr=3e-5)\n\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:13.412714Z","iopub.execute_input":"2024-10-05T08:50:13.413099Z","iopub.status.idle":"2024-10-05T08:50:59.726304Z","shell.execute_reply.started":"2024-10-05T08:50:13.413063Z","shell.execute_reply":"2024-10-05T08:50:59.725310Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a08112e3c5844ee81bbc4f1eedd5ccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d1cf261de3478e9c96edff8a8de928"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (embedding): Embedding(30522, 512)\n  (encoder_layers): ModuleList(\n    (0-5): 6 x Encoder(\n      (mha): MultiHeadAttention(\n        (w_k): Linear(in_features=512, out_features=512, bias=True)\n        (w_q): Linear(in_features=512, out_features=512, bias=True)\n        (w_v): Linear(in_features=512, out_features=512, bias=True)\n        (out): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (ff1): Linear(in_features=512, out_features=1024, bias=True)\n      (ff2): Linear(in_features=1024, out_features=512, bias=True)\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n  )\n  (decoder_layers): ModuleList(\n    (0-5): 6 x Decoder(\n      (mmha): MultiHeadAttention(\n        (w_k): Linear(in_features=512, out_features=512, bias=True)\n        (w_q): Linear(in_features=512, out_features=512, bias=True)\n        (w_v): Linear(in_features=512, out_features=512, bias=True)\n        (out): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (mha): MultiHeadAttention(\n        (w_k): Linear(in_features=512, out_features=512, bias=True)\n        (w_q): Linear(in_features=512, out_features=512, bias=True)\n        (w_v): Linear(in_features=512, out_features=512, bias=True)\n        (out): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (ff1): Linear(in_features=512, out_features=1024, bias=True)\n      (ff2): Linear(in_features=1024, out_features=512, bias=True)\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.2, inplace=False)\n    )\n  )\n  (position_encodings): PositionalEmbedding()\n  (linear): Linear(in_features=512, out_features=30522, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\nmodel = model.to('cuda')\nmodel.to(device)\n\nnum_epochs=1\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()\n\n        # Move tensors to the specified device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n\n        logits = model(src=input_ids, tgt=labels, src_mask=attention_mask)\n        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    avg_loss = epoch_loss / len(train_dataloader)\n    print(f\"training : Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n    \n    model.eval()\n    epoch_loss = 0\n    for batch in tqdm(validation_dataloader):\n\n        # Move tensors to the specified device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        logits = model(src=input_ids, tgt=labels, src_mask=attention_mask)\n        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n\n        epoch_loss += loss.item()\n\n    avg_loss = epoch_loss / len(validation_dataloader)\n    print(f\"validation : Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T08:50:59.728764Z","iopub.execute_input":"2024-10-05T08:50:59.729255Z","iopub.status.idle":"2024-10-05T09:05:02.148337Z","shell.execute_reply.started":"2024-10-05T08:50:59.729209Z","shell.execute_reply":"2024-10-05T09:05:02.147216Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/935 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n100%|██████████| 935/935 [13:08<00:00,  1.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"training : Epoch 1/1, Loss: 9.4469\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 165/165 [00:53<00:00,  3.08it/s]","output_type":"stream"},{"name":"stdout","text":"validation : Epoch 1/1, Loss: 9.4309\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference\nTraining a transformer model from scratch without pretrained weights is nearly impossible considering the large number of dataset needed and the time taken for get trained is very high so here I am leaving it with one epoch\n\nIncase of inference of any sentence as a imput to the transformer model you just need to pass it through the preprocess function that is defined above and can be passed it through the model will give back the output tokens\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}