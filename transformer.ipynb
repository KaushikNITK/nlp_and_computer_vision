{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:09.596964Z","iopub.status.busy":"2024-10-08T11:49:09.596532Z","iopub.status.idle":"2024-10-08T11:49:15.461450Z","shell.execute_reply":"2024-10-08T11:49:15.460480Z","shell.execute_reply.started":"2024-10-08T11:49:09.596923Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, TensorDataset, random_split, ConcatDataset, Dataset\n","from tqdm import tqdm\n","\n","from torch.nn.utils.rnn import pad_sequence\n","\n","from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:15.463609Z","iopub.status.busy":"2024-10-08T11:49:15.463167Z","iopub.status.idle":"2024-10-08T11:49:16.436627Z","shell.execute_reply":"2024-10-08T11:49:16.435707Z","shell.execute_reply.started":"2024-10-08T11:49:15.463566Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:16.438284Z","iopub.status.busy":"2024-10-08T11:49:16.437800Z","iopub.status.idle":"2024-10-08T11:49:18.966541Z","shell.execute_reply":"2024-10-08T11:49:18.965592Z","shell.execute_reply.started":"2024-10-08T11:49:16.438250Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa7fb1f3480b440bb0a45cac5fd8fecb","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4df3e151fff41dd842b39b7c77c5772","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ac519f2a9ad4faeb16938fe7bf6cdc7","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/419k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d13034f651c6411f9fc94a49a3474bfe","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"954bdf44458346dba12a611f676bb3a6","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["ds = load_dataset(\"openai/gsm8k\", \"main\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:18.969454Z","iopub.status.busy":"2024-10-08T11:49:18.968956Z","iopub.status.idle":"2024-10-08T11:49:18.976733Z","shell.execute_reply":"2024-10-08T11:49:18.975718Z","shell.execute_reply.started":"2024-10-08T11:49:18.969417Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'answer'],\n","        num_rows: 7473\n","    })\n","    test: Dataset({\n","        features: ['question', 'answer'],\n","        num_rows: 1319\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["ds"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:18.978687Z","iopub.status.busy":"2024-10-08T11:49:18.978238Z","iopub.status.idle":"2024-10-08T11:49:18.993068Z","shell.execute_reply":"2024-10-08T11:49:18.991992Z","shell.execute_reply.started":"2024-10-08T11:49:18.978639Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n"," 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["ds['train'][0]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:18.995007Z","iopub.status.busy":"2024-10-08T11:49:18.994654Z","iopub.status.idle":"2024-10-08T11:49:19.046566Z","shell.execute_reply":"2024-10-08T11:49:19.045610Z","shell.execute_reply.started":"2024-10-08T11:49:18.994974Z"},"trusted":true},"outputs":[],"source":["if torch.cuda.is_available(): \n","    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["# Multi-Head Attention"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:19.048319Z","iopub.status.busy":"2024-10-08T11:49:19.047958Z","iopub.status.idle":"2024-10-08T11:49:19.061002Z","shell.execute_reply":"2024-10-08T11:49:19.060175Z","shell.execute_reply.started":"2024-10-08T11:49:19.048259Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, emb_size, heads):\n","        super(MultiHeadAttention,self).__init__()\n","        self.heads=heads\n","        self.emb_size=emb_size\n","        self.head_dim=self.emb_size//self.heads\n","        self.w_k=nn.Linear(emb_size,emb_size, bias=False)\n","        self.w_q=nn.Linear(emb_size,emb_size, bias=False)\n","        self.w_v=nn.Linear(emb_size,emb_size, bias=False)\n","        self.out=nn.Linear(emb_size,emb_size)\n","\n","        assert(self.head_dim * heads == emb_size),\"embeding size is not divisible by number of heads\"\n","\n","\n","    def forward(self,k,q,v,mask=None):\n","        N=q.shape[0]  # batch size\n","        K=self.w_k(k)\n","        Q=self.w_q(q)\n","        V=self.w_v(v)\n","\n","        K=K.view(N,K.shape[1],self.heads,self.head_dim).transpose(1,2)    # (batch size, sequence len, heads, head dimention)\n","        Q=Q.view(N,Q.shape[1],self.heads,self.head_dim).transpose(1,2)    # transposed to give(batch size, heads, sequence len, head dimention)\n","        V=V.view(N,V.shape[1],self.heads,self.head_dim).transpose(1,2)\n","\n","        attention=(torch.matmul(Q,K.transpose(-2,-1)))/torch.tensor(self.head_dim**0.5)\n","        \n","        if mask is not None:\n","            mask=mask.reshape(-1,1,1,1024)\n","            attention.masked_fill_(mask==0, -1e9)\n","\n","        attention_scores=F.softmax(attention, dim=-1)\n","        output=torch.matmul(attention_scores,V)\n","        output = output.transpose(1, 2).reshape(N, -1, self.emb_size)    # combine all heads (batch_size, seqlen, dmodel)\n","        output=self.out(output)                                          # (batch_size, seqlen, dmodel)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# Transformer architecture"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:19.062203Z","iopub.status.busy":"2024-10-08T11:49:19.061932Z","iopub.status.idle":"2024-10-08T11:49:19.072825Z","shell.execute_reply":"2024-10-08T11:49:19.072030Z","shell.execute_reply.started":"2024-10-08T11:49:19.062174Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, heads, emb_size):\n","        super(Encoder, self).__init__()\n","        self.mha=MultiHeadAttention(emb_size, heads)\n","        self.ff1=nn.Linear(emb_size,2*emb_size)\n","        self.ff2=nn.Linear(2*emb_size, emb_size)\n","        self.norm1=nn.LayerNorm(emb_size)\n","        self.norm2=nn.LayerNorm(emb_size)\n","        self.dropout=nn.Dropout(p=0.2)\n","\n","    def forward(self, x, mask=None):\n","        attention_out=self.mha(x,x,x,mask)\n","        attention_out = self.dropout(attention_out)\n","        out1=self.norm1(x+attention_out)\n","\n","        ff_out=F.relu(self.ff1(out1))\n","        ff_out=self.ff2(ff_out)\n","        out2=self.dropout(ff_out)\n","        encoder_out=self.norm2(out1+out2)\n","        return encoder_out"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:19.074575Z","iopub.status.busy":"2024-10-08T11:49:19.074249Z","iopub.status.idle":"2024-10-08T11:49:19.084723Z","shell.execute_reply":"2024-10-08T11:49:19.083863Z","shell.execute_reply.started":"2024-10-08T11:49:19.074523Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, heads, emb_size):\n","        super(Decoder, self).__init__()\n","        self.mmha=MultiHeadAttention(emb_size, heads)\n","        self.mha=MultiHeadAttention(emb_size, heads)\n","        self.ff1=nn.Linear(emb_size,2*emb_size)\n","        self.ff2=nn.Linear(2*emb_size, emb_size)\n","        self.norm1=nn.LayerNorm(emb_size)\n","        self.norm2=nn.LayerNorm(emb_size)\n","        self.norm3=nn.LayerNorm(emb_size)\n","        self.dropout=nn.Dropout(p=0.2)\n","\n","    def forward(self, x, encoder_out, source_mask, target_mask):\n","        mask_attention_out=self.mmha(x,x,x,target_mask)\n","        mask_attention_out=self.dropout(mask_attention_out)\n","        out1=self.norm1(x+mask_attention_out)\n","\n","        enc_dec_attention_out=self.mha(encoder_out,out1,encoder_out)\n","        enc_dec_attention_out=self.dropout(enc_dec_attention_out)\n","        out2=self.norm2(out1+enc_dec_attention_out)\n","\n","        ff_output=F.relu(self.ff1(out2))\n","        ff_output=self.ff2(ff_output)\n","        ff_output=self.dropout(ff_output)\n","        out3=self.norm3(out2+ff_output)\n","\n","        return out3"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:19.088175Z","iopub.status.busy":"2024-10-08T11:49:19.087825Z","iopub.status.idle":"2024-10-08T11:49:19.096130Z","shell.execute_reply":"2024-10-08T11:49:19.095265Z","shell.execute_reply.started":"2024-10-08T11:49:19.088140Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(nn.Module):\n","    def __init__(self, seq_len, emb_size, n=10000):\n","        super(PositionalEmbedding, self).__init__()\n","        self.embedding = self.create_positional_embedding(seq_len, emb_size, n)\n","\n","    def create_positional_embedding(self, seq_len, emb_size, n):\n","        P = np.zeros((seq_len, emb_size))\n","        for pos in range(seq_len):\n","            for i in range(emb_size // 2):\n","                denominator = np.power(n, 2 * i / emb_size)\n","                P[pos, 2 * i] = np.sin(pos / denominator)\n","                P[pos, 2 * i + 1] = np.cos(pos / denominator)\n","        return torch.tensor(P, dtype=torch.float32)\n","\n","    def forward(self, idx):\n","        pos_idx=self.embedding\n","        pos_idx=pos_idx.to(device)\n","        idx=idx+pos_idx\n","        return idx"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:19.097620Z","iopub.status.busy":"2024-10-08T11:49:19.097260Z","iopub.status.idle":"2024-10-08T11:49:19.107842Z","shell.execute_reply":"2024-10-08T11:49:19.106852Z","shell.execute_reply.started":"2024-10-08T11:49:19.097573Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, vocab_size, input_dim, emb_size, num_encoder_layers, num_decoder_layers, heads, seq_len):\n","        super(Transformer, self).__init__()\n","        \n","        self.embedding=nn.Embedding(vocab_size, emb_size)\n","        self.encoder_layers = nn.ModuleList([Encoder(heads, emb_size) for _ in range(num_encoder_layers)])\n","        self.decoder_layers = nn.ModuleList([Decoder(heads, emb_size) for _ in range(num_decoder_layers)])\n","        self.position_encodings = PositionalEmbedding(seq_len, emb_size)\n","        self.linear = nn.Linear(emb_size, vocab_size)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n","\n","        src=self.embedding(src)   # need to do *sqrt(d_model)\n","        tgt=self.embedding(tgt)   # for transulation task give different embedding\n","\n","        src=self.position_encodings(src)\n","        tgt=self.position_encodings(tgt)\n","\n","        for encoder in self.encoder_layers:\n","            src = encoder(src, src_mask)\n","        \n","        for decoder in self.decoder_layers:\n","            tgt = decoder(tgt, src, src_mask, tgt_mask)\n","\n","        output = self.linear(tgt)\n","        output = F.softmax(output, dim=-1)\n","        \n","        return output\n","    \n","\n","\n","input_dim = 1000\n","emb_size = 512\n","heads = 8\n","num_encoder_layers = 6\n","num_decoder_layers = 6\n"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenization"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:19.109393Z","iopub.status.busy":"2024-10-08T11:49:19.108849Z","iopub.status.idle":"2024-10-08T11:49:20.106071Z","shell.execute_reply":"2024-10-08T11:49:20.104971Z","shell.execute_reply.started":"2024-10-08T11:49:19.109362Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71b355d1f64c42aa997a0624333ff533","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ea2357f488e4fb4a1b8817b75b5a4c4","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4942ba07f71402f9adc5743bbb2b011","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f0a40f8d6024cefa2c0a24df31c6aa6","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-08T11:49:22.112508Z","iopub.status.busy":"2024-10-08T11:49:22.112129Z","iopub.status.idle":"2024-10-08T11:50:07.607596Z","shell.execute_reply":"2024-10-08T11:50:07.606685Z","shell.execute_reply.started":"2024-10-08T11:49:22.112470Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69b3b7766c764c0ab72ac6fd0734549f","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a26857b4b1e4f48b58503c8c4e634ba","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Transformer(\n","  (embedding): Embedding(30522, 512)\n","  (encoder_layers): ModuleList(\n","    (0-5): 6 x Encoder(\n","      (mha): MultiHeadAttention(\n","        (w_k): Linear(in_features=512, out_features=512, bias=True)\n","        (w_q): Linear(in_features=512, out_features=512, bias=True)\n","        (w_v): Linear(in_features=512, out_features=512, bias=True)\n","        (out): Linear(in_features=512, out_features=512, bias=True)\n","      )\n","      (ff1): Linear(in_features=512, out_features=1024, bias=True)\n","      (ff2): Linear(in_features=1024, out_features=512, bias=True)\n","      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.2, inplace=False)\n","    )\n","  )\n","  (decoder_layers): ModuleList(\n","    (0-5): 6 x Decoder(\n","      (mmha): MultiHeadAttention(\n","        (w_k): Linear(in_features=512, out_features=512, bias=True)\n","        (w_q): Linear(in_features=512, out_features=512, bias=True)\n","        (w_v): Linear(in_features=512, out_features=512, bias=True)\n","        (out): Linear(in_features=512, out_features=512, bias=True)\n","      )\n","      (mha): MultiHeadAttention(\n","        (w_k): Linear(in_features=512, out_features=512, bias=True)\n","        (w_q): Linear(in_features=512, out_features=512, bias=True)\n","        (w_v): Linear(in_features=512, out_features=512, bias=True)\n","        (out): Linear(in_features=512, out_features=512, bias=True)\n","      )\n","      (ff1): Linear(in_features=512, out_features=1024, bias=True)\n","      (ff2): Linear(in_features=1024, out_features=512, bias=True)\n","      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.2, inplace=False)\n","    )\n","  )\n","  (position_encodings): PositionalEmbedding()\n","  (linear): Linear(in_features=512, out_features=30522, bias=True)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Preprocess data\n","def preprocess_function(examples):\n","    # Combine question and answer for input\n","    inputs = [f\"question: {q} answer:\" for q in examples['question']]\n","    targets = examples['answer']\n","    \n","    model_inputs = tokenizer(inputs, truncation=True, padding='max_length', max_length=1024)\n","    labels = tokenizer(targets, truncation=True, padding='max_length', max_length=1024)\n","    \n","    model_inputs['labels'] = labels['input_ids']\n","    return model_inputs\n","\n","# Split dataset\n","train_data = ds['train'].map(preprocess_function, batched=True) \n","test_data = ds['test'].map(preprocess_function, batched=True)\n","\n","\n","class QNADataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': torch.tensor(self.data[idx]['input_ids']),\n","            'attention_mask': torch.tensor(self.data[idx]['attention_mask']),\n","            'labels': torch.tensor(self.data[idx]['labels'])\n","        }\n","\n","train_dataset = QNADataset(train_data)\n","test_dataset = QNADataset(test_data)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=4)\n","validation_dataloader = DataLoader(test_dataset, batch_size=8, pin_memory=True, num_workers=4)\n","\n","model = Transformer(tokenizer.vocab_size, input_dim, emb_size, num_encoder_layers, num_decoder_layers, heads, seq_len=1024)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","optimizer = optim.Adam(model.parameters(), lr=3e-5)\n","\n","model"]},{"cell_type":"markdown","metadata":{},"source":["# Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text=\"hi how are yous\""]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-05T08:50:59.729255Z","iopub.status.busy":"2024-10-05T08:50:59.728764Z","iopub.status.idle":"2024-10-05T09:05:02.148337Z","shell.execute_reply":"2024-10-05T09:05:02.147216Z","shell.execute_reply.started":"2024-10-05T08:50:59.729209Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Let's use 2 GPUs!\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/935 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","100%|██████████| 935/935 [13:08<00:00,  1.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["training : Epoch 1/1, Loss: 9.4469\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 165/165 [00:53<00:00,  3.08it/s]"]},{"name":"stdout","output_type":"stream","text":["validation : Epoch 1/1, Loss: 9.4309\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","if torch.cuda.device_count() > 1:\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model)\n","model = model.to('cuda')\n","model.to(device)\n","\n","num_epochs=1\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for batch in tqdm(train_dataloader):\n","        optimizer.zero_grad()\n","\n","        # Move tensors to the specified device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass\n","\n","        logits = model(src=input_ids, tgt=labels, src_mask=attention_mask)\n","        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    avg_loss = epoch_loss / len(train_dataloader)\n","    print(f\"training : Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","    \n","    model.eval()\n","    epoch_loss = 0\n","    for batch in tqdm(validation_dataloader):\n","\n","        # Move tensors to the specified device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass\n","        logits = model(src=input_ids, tgt=labels, src_mask=attention_mask)\n","        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n","\n","        epoch_loss += loss.item()\n","\n","    avg_loss = epoch_loss / len(validation_dataloader)\n","    print(f\"validation : Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Inference\n","Training a transformer model from scratch without pretrained weights is nearly impossible considering the large number of dataset needed and the time taken for get trained is very high so here I am leaving it with one epoch\n","\n","Incase of inference of any sentence as a imput to the transformer model you just need to pass it through the preprocess function that is defined above and can be passed it through the model will give back the output tokens\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text=\"hi how are you\"\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
